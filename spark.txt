
grep ->hadoop -> spark 

big code -> tiny code

spark --- readability  (tinier code)
		  expressiveness (more expressive code)
		  fast ()
		  testability (you can write code isnt distibute)
		  interactive (you directly interect from local machine -- you can debug more problems from local machine)
		  fault tolerance ()
		  unify big data needs (ML , graph processing, streaming data etc) 
basics of spark 
core API
cluster managers
spark maintenance
libraries

--- spark bg--
for streaming - storm 
scalding
clear data - hive
so so so so so so 

to overcome all these frameworks spark was introduced

aa unified paltform

DataFrames 
|
|
spark-------spark   -----machinelearning----graphx
sql 		streaming 
|
|
spark core


Lines of code in decresing manner
hadoop MR->Impala -> Storm -> Giraph-> Spark
say
100000->80000->70000->60000->75000(40000(spark core),10000(SQL which used for optimisation),10000(Streaming data),5000(GraphX) Which includes all the previous codes in one set)


There is no need to write intermediate data into disk which is very costly

all in memory 

we can learn one framework instead og many 



---------

MR 2004 -> hadoop 2006 -> spark 2009 -> spark paper 2010 ,BSD open source -> incubated amplab 2011 -> databricks 2013 for supporting spark , Apache  -> 2014 apache top level project  

Databricks -- stable framework 

who is using

pandora, yahoo,netflix, goldman sachs, comcast,conviva
ooyala, ebay,twitter,janelia , 

Spark-2356 ticket will help us in removing error  during starting of spark-shell in windows


this ticket will provie winutil.exe

------------------

spark supports

scala (it will be best )
java
python
R

------------
REPL read execute present Loop

shell creates sc(spark context) and sqlContext(sql lib)

sc is the starting point

RDD - resilient distributed Dataset

spark core divided to transformation and actions 

Spark is lazy evluation

val textfile = sc.textFile("Readme.md")
textFile.first

data.distinct == where distinct takes only distinct wore based on hash and equality methods for removing duplicates

sample data would not be sent back to drive 

sparks makes unit testing more easy than any programming model

core API
-Appify
-RDD
-Transforming
-Action1

Driver is managed by spark context
|\
|  \
worer   worker 

sc  --- builds execution graph which sends to each worker 
Task Creator
scheduler
data locality
fault tolerance (monitoring those tasks)

There are more multiple ways to create one or more SC

but this is not encouraged becuase of discripency in managing 

RDD - Resilient Distributed Dataset
collection of elements paritioned across nodes of the cluster that can be operated on in parallel

resilient - fault tolerance
distributed - process in distributed statement


DAG files are created storing all the statements , until a action statement is called 

Directed Acyclic Graph DAG

DAG are sent to workers

Transformations
  map
  filter
  
  
  Actions
   collect
   count
   reduce
 
RDD is immutable , this is a lineage ... eac action triggers fresh execution of DAG

excuted parallel across CPU
input----
file:// 
hdfs://
s3n://
from DB
even we can load from memory 
different file formats (avro , parquet)

lamda expressions also known as anonymous functions 

rdd.flatMap(line=>line.split(" "))  -----> 

python allows single line lamdas 

spark can support multiple line lamdas functions 

Transformations : 

spark RDD has two types of methods -- transformation and actions 

1 RDD to other RDD is Transforming

mapItemFunctions(items) will be more usefull when map the items 

Araay -- RDD of arrays 

combinedRDD=RDD1.union(RDD2)

you can follow union distinct 
we can use ++
sc.union(RDD1,RDD2)

RDD1.intersection(RDD2)

RDD1.subtract(RDD2)

RDD1.cartesian(RDD2) = which is like double for loop = iterate between two RDD's 
here each element will be linked to all the elements of other RDD

RDD1.zip(RDD2) --> requires both rdd's should have same number of elements or paritions
each item in each RDD should be paired or should have pair ----
zipwithIndex zipwithUniqueID  will be no use

this will be more usefull 

Actions : 
actions results typically to send the data back to driver after executing 

if the actions ran in each worker before sending the data back to driver this is map side combine 

here we reuire function parameters should be associative 

(2+4)+(4+7)
6+11
17


and (2+4+4)+7
10+7
17

.sample(false,fraction=.01) == slect 1% of data from RDD where fraction rage is from 0 to 1
.take(5) == select 5 elements of rdd` 
.takesample(false,5) == selection randon elements od rdd
.max
.min

spark-shell --jars "list of jars" loding jards


.COUNTAPPROXDISTINCT(.01) OR (.0005) THE HIGHER THE VALUE , THE HIGHER THE SPEED 
THE LOWER VALUE , THE HIGHER THE CORRECTNESS 

MAXBY(x=>x._2)

val simpleRDD=sc.makeRDD(1 to 3)
simpleRDD.reduce((acc,val)=>acc+val)

reduce is associative and commutative 

sc.emptyRDD[Int].reduce((x,y)=>x+y) --> unsupported operation exception becaue RDD is empty

simpleRDD.fold(0)(x,y)=>x+y  
			  ---
			   |
			   |
			 this value gives what output it should result like  0 is Int so res0: Int=6
simpleRDD.aggregate("")(()=>,()=>)
			        ---
			         |
			         |
			 this value gives what output it should result like  "" is String so res0: String=213
			 
	this aggregate function will yield different values when you run the same statment,. 
	
finala action 
Data Persistance : 

in big data where 
from workers we can put/ write data direclty to data souce 

saveAsObjectFile(path)
saveAsTextFile(path)  = two string method 
External connector == save natively in whatever format you want
foreach(T=>Unit)
  foreachpartition(Iterator[T]=>Unit)
  
Implicit conversions: 

1.plus(1) --- there is no plus method  // gives error to rectify that we have to write explicit function 

case class IntExtensions(value:Int){
def plus(operand:Int)=value+operand
}

IntExtensions)(1).plus(1) --- executes 

but which is not more precise ....  to do that we have to import Implicitconversions package
 
import scala.language.implicitConversions

 implicit def inttoIntExtensions(value:Int)={   // give impicit keyword like this
	IntExtensions(value)
 }

 1.plus(1) --- now it will run --- it will search for all implicit methods that matches our paramethertypes
 
 for older version we have to import sparkContext._
 
 RDD implicits 
 like 
 doubleRDDtoDoubleRDDFunctions
 etc
 
 rddtoPaiRrddFucntions ---- python has more functions 
 pair data 
 say we have
 same keys and different vlaues
 
 when we run a pair type operation, all the same type of keys will be partitioned to same node , which is like same key will reside in one parition and in one machine only
 
 key value Methods :
 
 collectAsMap
 mapValues
 flatMapvalues
 reduceBykey (it is a transformation)
 foldBykey
 aggregatebykey
 combineBykey
 groupByKey
 countByKey
 CountApproxDinstinctByKey
 sampleByKey
 subtractByKey
 sortByKey

we have SQL like pairings

and other ones are cogroup and groupWith

pair saving 

SaveAshadoopFile
saveAsHadoopDataSet
savaAsSequenceFile( we have in scala and python)

Cache -- 

we can store intermediate data in memeory or disk or both memeory and disk
cache/persist
default storageLevel is Memory_Only
persist(newLevel:SotrageLevel)
Memory_only 
Disk_only
Memory_Only_ser (storing data serialize)
.....
OFF_Heap (sored data in jvm heap)

unpersist(blocking:Boolean=true)

Accumulator

to catch the tangential data from the RDD operations like Errors

we have this accumulator

val accumulator = sc.accumulator(0,"Accumulator Name")

accumalor creation and writing will be done only in Driver

cluster Mangers spark maintanence
------------------------------------
spark-submit - launches a driver-> main method ---> cluster manager ---> workers
										|									^|	
										|_____ driver create RDD and RUNRDD__|
 
spark-submit kill or status can be taken (v 1.3 later )

spark-submit --help

Cluster Manager -- like a distributed kernal -- it is a single machine

manage machines and tasks 

Kernal for monitoring the data center 

Spark cluster managers

basic builtin  stand alone --- --master spark://[HOST]:7077 -- client or cluster modes -- spark.deploy.spreadOut =true/false  // if true the process will distributedly else only in one single machin
-total-executor-cores # // default to use as many cores as poscible for processing
-executor-cores

----

 for hadoop it is yarn
 
 --master yarn
 
 Hadoop/YARN_CONF_DIR
 cline/cluster /  we can give delpoyenment giving -delopyment option after yarn 
 
 -num-executors 2 default 
-executor-cores
-queues default
 
 
 
 -----------
 another mesos cluster manager 
 --master mesos://[HOST]:5050
 client/(cluster)
 start mesos-dispatch.sh 
 
 it dynamically allocate resources and assign cores
 spark.mesos.coarse=false 
 
 no spreadOut option
 
 
 =========================
 
 spark standalone
 
 $SPARK_HOME
 conf/slaves
 
 ./sbin/start-all.sh    ssh 
 
 this wont work in windows 
 to lucnh we have manual start each machine like below 
 
 >bin/spark-class org.apache.spark.deploy.master.Master
 >bin/spark-class org.apache.spark.deploy.worer.Worker spark://[MASTER]:7077
 
we stop manually 
	
conf/spark-env.sh   --> export SPARK_MASTER_OPTS=defaultcores=<values>

--- with amazon 
cd spark/ec2  --> we need keys 
python spark_ec2.py --help

spark libraries : 

SQL , Streaming, MLib/Graphx

SPARK SQL: 

it meets sql 92 standardss

datasource can be hive,json,parquet,  etc

optimization
 predicate push down - optimize engine can check whether we can push filter under 		shuffling operators
 coumn pruning --  analysers can drop unneccesory columns 
Uniform API  -- same jvm fro any code 
Code generation== performance gains 
sql->RDD->sql

import SqlContext.implicits._


----
registerTempTable 

sql()

we can cache 

and cache lazy

sTREAMNING: 

PRIMARy competiotion is Storm (data can be duplicated , process each item one by one)



40 time of storm
prcocess the items as a bacthc called micro batch

kafka  |
flume  |--> spark streaming ----RDD RDD  RDD ----> spark ----> 
hdfs   |
twitter|


serialization because of reducing garbage data

default cahcing is memory only 

MLib ;

Matlab , R , mahout,GraphLab

MLib came freom ML ooptimiser -> MLib these help in testing MLib 

org.apache.spark.mlib

breaking , testing, tuning, testing, failure detetection etc 


Algorithms : 
 classification 
 regression
 collaborative filtering
 clustering
 dimentional reduction 
Feature Extraction and Transformations

spam filetering, fraud detecting ... 

Graphx 

 It contains verticces ,edges and information realted with vertices  called Triplets

Graphx(VType,EType) --> Graphx(RDD(Vertex),RDD(Edges))  
EdgeTriplet -- contains all the information about connection 





 The last construct is the Scala object, the singleton static class, such as our FileConverter class and its converToIOObject; again there's some mingling that occurs. So, to use this in Java, we need to take the class name with an appended $ sign, and call its all caps MODULE$ variable. This gives us a final reference that we can use to call our convertToIOObject method directly. Again, not exactly pretty due to the name mangling, but it does the trick.
 
 to use scala in java 
 
 trait IOObject {
 val file:File
 val name={
 file.getName()
 }
  }

  hello worlds
  

 java program to use above trait
 
 public class FileObject implements IOObject 
 {
 
 public File file(){
 
 return localFile;
 }
 public String name(){
 return IOObject$class.name();
 }
 
 }
 
 
 ------
 working with singleton objects ": 

 object FileConverter{
 
 def converToIOObject(file:File) = ...
 
 }
	||
	||
 
 in java we have to us Object as 
 
 FileConverter$.MODULE$.converToIOObject(file);
 

REPL = read evluate present loop

scalatest library and selecting testing style oof coding 

xUnit(AAA) ----->FunSuite   /// AAA -- Arrange,Act,Assert 
BDD ----------->FlatSpec //BDD Behaviour driven developemet
RSpec ------> FlatSuit   //for ruby 
acceptance ----->FeatureSpec // for acceptance testing

now FileSearcher is codded in BDD 

we have to extend FlatSpec 

FlatSpec is a Scala trait . we can think trait as a abstract class

org.scalatest._   // _ is a wildcard which makes the whole package avalaible to us

== object eqality 
.eq for ref equality

in java it is vice versa

~test

when an object is created with the same name and in the same file as another class then it 
is referred to as a companion object 

FilterCheck.scala
--------------
class FilterCheck{

}
object FilterCheckheck{

def apply ()  ==== // create a apply method which creates a instance of FilterCheck 
}

=========== 
FilterCheck is a companion object 
 
 
infix notation where we remove . (   ) extra make it more expressive clean code 

case class constructor arguments are always public and they always come with comoanion object i.e., which have apply and unapply methods

??? -- not impelented any  --- throws non implemented exceptions	
case _ -- default one

a method is considered tail recusrive if it calls itself as its a last action

to join single element to list ::
to join to lists :::

tuple stores 22 diff values

play framework: scala

play 1 2009 written in Java and support scala lib---  ivy --- goal centered and building apps  
play 2 2012 written in Scala  ----- sbt  ---- focus on building apps that are scalable ,lightweight,satateless asyinchromus immutable approach


spark-env.sh -- contains hadoop propertieds 

val sc=new SparkContext(new SparkConf())

spark.speculation = true --- it relunches the slow running tasks , which is called speculation

general RDD functions: 
cache
collect
count
countByValue
distinctfileter
foreach
persist
count
sample
union

transformations:

map , filter ,intersection, coalease, cogroup

actions:
reduce,first,foreach,savaAsSequenceFile etc..

pairRDDFunctions: 
lookup
mapValues
CollectAsMap

val doubleRDD= sc.parallelize(Seq(1,4,5,3,23,65,6,64,326,9,745,53,2,2,6,))
doubleRDD.mean , doubleRDD.sum , doubleRDD.min , dubleRDD.max , doubleRDD.variance 

out of all this operations doubleRDD.stats will print all the above fields

for joins in spark we need key value pairs .. 

joins with partition will increase huge performance.. like 
the partitioned data will not move only other datasets will move across specified partitions to join 

where as in normal joins both will move and join 

default partitioner is Hashpartitioner 

partitionBy(new HashPartitioner(2)) // 2 is no. of partitions
-----
streaingContext(sparkConf,Seconds(15))

ssc.start
ssc.awaitTermination

terminal cmd nc -lk 9999

it will not remeber the previous streams of data which was caught

streaming is stateless.... where batch 2 dunno about the batch 1 data

ssc.filestream(dir) -- it will catch the new files not the modified one

rolling window --  which is processing the incompleted  datastreams upto that time period
sliding window --  process the datastreams at that time period

in spark we have window operations 

we have to use reduceByKeyAndWindow(_+_,Seconds(30))  /// this 30 sould be multiples of batch window say 2*15



checkpointing: for fault tolerance in streaming
it allows streamging data to be stored locally in hdfs or s3

metadata(incomplete datastreams,Dstream operations,conf) will be store at checkpoints

MLlib
-------
alogirithms and utilities , classificagtion, regression,clustering,collaborative filtering , etc... 

spark.mllib -- original lib API built on top of RDD -- 
spark.ml ---  original highet level API built on top of dataframes for constructing ML pipelines.

Spark ML Concepts:
DataFrame - ML uses Sql DataFrame as Dataset which can hold variety of datatypes
Transformer - is an algorithm to transform one dataframe to another dataframe
Estimator - is an algorithm which van be fit on a dataframe to produce dataframe
Pipeline - chain of transformers and estimators
Parameter - parmeters for these transformers and estimators


mllib on dataframes is experimental 

ML pipeline  or process

featurization - > training - > Model evalution


ML pipeline
						MLlib
						|  |
datasource -> hadoop ->spark ->application

org.apache.spark.mllib.recommendation.ALS



mysql provides multisession processing ... so avoid derby for running both hql and spark sql 


in spark -sql where schema rdd to dataframe  evloved from older to newer version

when you change the metastore of hive to mysql ...  while starting the spark shell you have to load the mysql connector along

val ppl= sc.textFile().toDF
ppl.show


val pplq= sc.read.json()
pplq.show

pplq.printSchema

pplq.registerTempTable("pplq")

val ppd=SqlContext.sql(select name , age,-- from pplq) // check with sc
ppd.foreach(println)

sqlContext.parquetFile()

we have to take hive-site.xml from hive/conf to spark/conf--- which eventually connects the spark and hive

accumalors can be added but not subtract/decrease

broadcast values can be sent to all node using broadcast alogarithm ... 

here accumalators wiill be sent back to driver then they will be added

acc.value gives the value of accumulator 

sc.accumalator(0)

first filter then join ,aggregate, cogroup ,

GraphX

for graphs and graph parallel computation

vertex property 
edge property










 
 




















