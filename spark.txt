
grep ->hadoop -> spark 

big code -> tiny code

spark --- readability  (tinier code)
		  expressiveness (more expressive code)
		  fast ()
		  testability (you can write code isnt distibute)
		  interactive (you directly interect from local machine -- you can debug more problems from local machine)
		  fault tolerance ()
		  unify big data needs (ML , graph processing, streaming data etc) 
basics of spark 
core API
cluster managers
spark maintenance
libraries

--- spark bg--
for streaming - storm 
scalding
clear data - hive
so so so so so so 

to overcome all these frameworks spark was introduced

aa unified paltform

DataFrames 
|
|
spark-------spark   -----machinelearning----graphx
sql 		streaming 
|
|
spark core


Lines of code in decresing manner
hadoop MR->Impala -> Storm -> Giraph-> Spark
say
100000->80000->70000->60000->75000(40000(spark core),10000(SQL which used for optimisation),10000(Streaming data),5000(GraphX) Which includes all the previous codes in one set)


There is no need to write intermediate data into disk which is very costly

all in memory 

we can learn one framework instead og many 



---------

MR 2004 -> hadoop 2006 -> spark 2009 -> spark paper 2010 ,BSD open source -> incubated amplab 2011 -> databricks 2013 for supporting spark , Apache  -> 2014 apache top level project  

Databricks -- stable framework 

who is using

pandora, yahoo,netflix, goldman sachs, comcast,conviva
ooyala, ebay,twitter,janelia , 

Spark-2356 ticket will help us in removing error  during starting of spark-shell in windows


this ticket will provie winutil.exe

------------------

spark supports

scala (it will be best )
java
python
R

------------
REPL read execute present Loop

shell creates sc(spark context) and sqlContext(sql lib)

sc is the starting point

RDD - resilient distributed Dataset

spark core divided to transformation and actions 

Spark is lazy evluation

val textfile = sc.textFile("Readme.md")
textFile.first

data.distinct == where distinct takes only distinct wore based on hash and equality methods for removing duplicates

sparks makes unit testing more easy than any programming model

core API
-Appify
-RDD
-Transforming
-Action1

Driver is managed by spark context
|\
|  \
worer   worker 

sc  --- builds execution graph which sends to each worker 
Task Creator
scheduler
data locality
fault tolerance (monitoring those tasks)

There are more multiple ways to create one or more SC

but this is not encouraged becuase of discripency in managing 

RDD - Resilient Distributed Dataset
collection of elements paritioned across nodes of the cluster that can be operated on in parallel

resilient - fault tolerance
distributed - process in distributed statement


DAG files are created storing all the statements , until a action statement is called 

Directed Acyclic Graph DAG

DAG are sent to workers

Transformations
  map
  filter
  
  
  Actions
   collect
   count
   reduce
 
RDD is immutable , this is a lineage ... eac action triggers fresh execution of DAG

excuted parallel across CPU
input----
file:// 
hdfs://
s3n://
from DB
even we can load from memory 
different file formats (avro , parquet)

lamda expressions also known as anonymous functions 

rdd.flatMap(line=>line.split(" "))  -----> 

python allows single line lamdas 

spark can support multiple line lamdas functions 

Transformations : 

spark RDD has two types of methods -- transformation and actions 

1 RDD to other RDD is Transforming

mapItemFunctions(items) will be more usefull when map the items 

Araay -- RDD of arrays 

combinedRDD=RDD1.union(RDD2)

you can follow union distinct 
we can use ++
sc.union(RDD1,RDD2)

RDD1.intersection(RDD2)

RDD1.subtract(RDD2)

RDD1.cartesian(RDD2) = which is like double for loop = iterate between two RDD's 
here each element will be linked to all the elements of other RDD

RDD1.zip(RDD2) --> requires both rdd's should have same number of elements or paritions
each item in each RDD should be paired or should have pair ----
zipwithIndex zipwithUniqueID  will be no use

this will be more usefull 

Actions : 
actions results typically to send the data back to driver after executing 

if the actions ran in each worker before sending the data back to driver this is map side combine 

here we reuire function parameters should be associative 

(2+4)+(4+7)
6+11
17


and (2+4+4)+7
10+7
17

.sample(false,fraction=.01) == slect 1% of data from RDD where fraction rage is from 0 to 1
.take(5) == select 5 elements of rdd` 
.takesample(false,5) == selection randon elements od rdd
.max
.min

spark-shell --jars "list of jars" loding jards


.COUNTAPPROXDISTINCT(.01) OR (.0005) THE HIGHER THE VALUE , THE HIGHER THE SPEED 
THE LOWER VALUE , THE HIGHER THE CORRECTNESS 

MAXBY(x=>x._2)

val simpleRDD=sc.makeRDD(1 to 3)
simpleRDD.reduce((acc,val)=>acc+val)

reduce is associative and commutative 

sc.emptyRDD[Int].reduce((x,y)=>x+y) --> unsupported operation exception becaue RDD is empty

simpleRDD.fold(0)(x,y)=>x+y  
			  ---
			   |
			   |
			 this value gives what output it should result like  0 is Int so res0: Int=6
simpleRDD.aggregate("")(()=>,()=>)
			        ---
			         |
			         |
			 this value gives what output it should result like  "" is String so res0: String=213
			 
	this aggregate function will yield different values when you run the same statment,. 
	
finala action 
Data Persistance : 

in big data where 
from workers we can put/ write data direclty to data souce 

saveAsObjectFile(path)
saveAsTextFile(path)  = two string method 
External connector == save natively in whatever format you want
foreach(T=>Unit)
  foreachpartition(Iterator[T]=>Unit)
  
Implicit conversions: 

1.plus(1) --- there is no plus method  // gives error to rectify that we have to write explicit function 

case class IntExtensions(value:Int){
def plus(operand:Int)=value+operand
}

IntExtensions)(1).plus(1) --- executes 

but which is not more precise ....  to do that we have to import Implicitconversions package
 
import scala.language.implicitConversions

 implicit def inttoIntExtensions(value:Int)={   // give impicit keyword like this
	IntExtensions(value)
 }

 1.plus(1) --- now it will run --- it will search for all implicit methods that matches our paramethertypes
 
 for older version we have to import sparkContext._
 
 RDD implicits 
 like 
 doubleRDDtoDoubleRDDFunctions
 etc
 
 rddtoPaiRrddFucntions ---- python has more functions 
 pair data 
 say we have
 same keys and different vlaues
 
 when we run a pair type operation, all the same type of keys will be partitioned to same node , which is like same key will reside in one parition and in one machine only
 
 key value Methods :
 
 collectAsMap
 mapValues
 flatMapvalues
 reduceBykey (it is a transformation)
 foldBykey
 aggregatebykey
 combineBykey
 groupByKey
 countByKey
 CountApproxDinstinctByKey
 sampleByKey
 subtractByKey
 sortByKey

we have SQL like pairings

and other ones are cogroup and groupWith

pair saving 

SaveAshadoopFile
saveAsHadoopDataSet
savaAsSequenceFile( we have in scala and python)

Cache -- 

we can store intermediate data in memeory or disk or both memeory and disk
cache/persist
default storageLevel is Memory_Only
persist(newLevel:SotrageLevel)
Memory_only 
Disk_only
Memory_Only_ser (storing data serialize)
.....
OFF_Heap (sored data in jvm heap)

unpersist(blocking:Boolean=true)

Accumulator

to catch the tangential data from the RDD operations like Errors

we have this accumulator

val accumulator = sc.accumulator(0,"Accumulator Name")

accumalor creation and writing will be done only in Driver

cluster Mangers spark maintanence
------------------------------------
spark-submit - launches a driver-> main method ---> cluster manager ---> workers
										|									^|	
										|_____ driver create RDD and RUNRDD__|
 
spark-submit kill or status can be taken (v 1.3 later )

spark-submit --help

Cluster Manager -- like a distributed kernal -- it is a single machine

manage machines and tasks 

Kernal for monitoring the data center 

Spark cluster managers

basic builtin  stand alone --- --master spark://[HOST]:7077 -- client or cluster modes -- spark.deploy.spreadOut =true/false  // if true the process will distributedly else only in one single machin
-total-executor-cores # // default to use as many cores as poscible for processing
-executor-cores

----

 for hadoop it is yarn
 
 --master yarn
 
 Hadoop/YARN_CONF_DIR
 cline/cluster /  we can give delpoyenment giving -delopyment option after yarn 
 
 -num-executors 2 default 
-executor-cores
-queues default
 
 
 
 -----------
 another mesos cluster manager 
 --master mesos://[HOST]:5050
 client/(cluster)
 start mesos-dispatch.sh 
 
 it dynamically allocate resources and assign cores
 spark.mesos.coarse=false 
 
 no spreadOut option
 
 
 =========================
 
 spark standalone
 
 $SPARK_HOME
 conf/slaves
 
 ./sbin/start-all.sh    ssh 
 
 this wont work in windows 
 to lucnh we have manual start each machine like below 
 
 >bin/spark-class org.apache.spark.deploy.master.Master
 >bin/spark-class org.apache.spark.deploy.worer.Worker spark://[MASTER]:7077
 
we stop manually 
	
conf/spark-env.sh   --> export SPARK_MASTER_OPTS=defaultcores=<values>

--- with amazon 
cd spark/ec2  --> we need keys 
python spark_ec2.py --help

spark libraries : 

SQL , Streaming, MLib/Graphx

SPARK SQL: 

it meets sql 92 standardss

datasource can be hive,json,parquet,  etc

optimization
 predicate push down - optimize engine can check whether we can push filter under 		shuffling operators
 coumn pruning --  analysers can drop unneccesory columns 
Uniform API  -- same jvm fro any code 
Code generation== performance gains 
sql->RDD->sql

import SqlContext.implicits._


----
registerTempTable 

sql()

we can cache 

and cache lazy

sTREAMNING: 

PRIMARy competiotion is Storm (data can be duplicated , process each item one by one)



40 time of storm
prcocess the items as a bacthc called micro batch

kafka  |
flume  |--> spark streaming ----RDD RDD  RDD ----> spark ----> 
hdfs   |
twitter|


serialization because of reducing garbage data

default cahcing is memory only 

MLib ;

Matlab , R , mahout,GraphLab

MLib came freom ML ooptimiser -> MLib these help in testing MLib 

org.apache.spark.mlib

breaking , testing, tuning, testing, failure detetection etc 


Algorithms : 
 classification 
 regression
 collaborative filtering
 clustering
 dimentional reduction 
Feature Extraction and Transformations

spam filetering, fraud detecting ... 

Graphx 

 It contains verticces ,edges and information realted with vertices  called Triplets

Graphx(VType,EType) --> Graphx(RDD(Vertex),RDD(Edges))  
EdgeTriplet -- contains all the information about connection 








	
 
 





 

 
 




















